{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://bac044c6c4c0:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f930ddd3580>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import mlflow\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.config(\"spark.jars.packages\", \"org.mlflow:mlflow-spark:1.11.0\")\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "mlflow.pyspark.ml.autolog()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|category|\n",
      "+---+--------+\n",
      "|  0|       a|\n",
      "|  1|       b|\n",
      "|  2|       c|\n",
      "|  3|       a|\n",
      "|  4|       a|\n",
      "|  5|       c|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [(0, 'a'), (1, 'b'), (2, 'c'), (3, 'a'), (4, 'a'), (5, 'c')],\n",
    "    ['id', 'category'], \n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/06/06 00:28:11 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'dbaf062e19d849f398302591de8ddc3e', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current pyspark.ml workflow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StringIndexerModel: uid=StringIndexer_6fd0a5b26d7a, handleInvalid=error"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol='category', outputCol='category_index')\n",
    "indexer_model = indexer.fit(df)\n",
    "indexer_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------------+\n",
      "| id|category|category_index|\n",
      "+---+--------+--------------+\n",
      "|  0|       a|           0.0|\n",
      "|  1|       b|           2.0|\n",
      "|  2|       c|           1.0|\n",
      "|  3|       a|           0.0|\n",
      "|  4|       a|           0.0|\n",
      "|  5|       c|           1.0|\n",
      "+---+--------+--------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(DataFrame[id: bigint, category: string, category_index: double], None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed_df = indexer_model.transform(df)\n",
    "indexed_df, indexed_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------------+-----------------+\n",
      "| id|category|category_index|original_category|\n",
      "+---+--------+--------------+-----------------+\n",
      "|  0|       a|           0.0|                a|\n",
      "|  1|       b|           2.0|                b|\n",
      "|  2|       c|           1.0|                c|\n",
      "|  3|       a|           0.0|                a|\n",
      "|  4|       a|           0.0|                a|\n",
      "|  5|       c|           1.0|                c|\n",
      "+---+--------+--------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import IndexToString\n",
    "\n",
    "converter = IndexToString(inputCol='category_index', outputCol='original_category')\n",
    "converted = converter.transform(indexed_df)\n",
    "converted.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---------+\n",
      "| id|category1|category2|\n",
      "+---+---------+---------+\n",
      "|  0|        a|        A|\n",
      "|  1|        b|        A|\n",
      "|  2|        c|        K|\n",
      "|  3|        a|        D|\n",
      "|  4|        a|        C|\n",
      "|  5|        c|        B|\n",
      "+---+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [(0, 'a', 'A'), (1, 'b', 'A'), (2, 'c', 'K'), (3, 'a', 'D'), (4, 'a', 'C'), (5, 'c', 'B')],\n",
    "    ['id', 'category1', 'category2'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/06/06 00:28:14 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '7416f2b3d2ea4920943a8e041fedc3d5', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current pyspark.ml workflow\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---------+--------------+--------------+\n",
      "| id|category1|category2|label_encoded1|label_encoded2|\n",
      "+---+---------+---------+--------------+--------------+\n",
      "|  0|        a|        A|           0.0|           0.0|\n",
      "|  1|        b|        A|           2.0|           0.0|\n",
      "|  2|        c|        K|           1.0|           4.0|\n",
      "|  3|        a|        D|           0.0|           3.0|\n",
      "|  4|        a|        C|           0.0|           2.0|\n",
      "|  5|        c|        B|           1.0|           1.0|\n",
      "+---+---------+---------+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexer = StringIndexer(inputCols=['category1', 'category2'], outputCols=['label_encoded1', 'label_encoded2'])\n",
    "indexed_model = indexer.fit(df)\n",
    "indexed_df = indexed_model.transform(df)\n",
    "indexed_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/06/06 00:28:16 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '5dfa5db91d034a4dbc1d2ec224ab4006', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current pyspark.ml workflow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+---------------+---------------+\n",
      "|categoryIndex1|categoryIndex2|onehot_encoded1|onehot_encoded2|\n",
      "+--------------+--------------+---------------+---------------+\n",
      "|           0.0|           1.0|  (2,[0],[1.0])|  (2,[1],[1.0])|\n",
      "|           1.0|           0.0|  (2,[1],[1.0])|  (2,[0],[1.0])|\n",
      "|           2.0|           1.0|      (2,[],[])|  (2,[1],[1.0])|\n",
      "|           0.0|           2.0|  (2,[0],[1.0])|      (2,[],[])|\n",
      "|           0.0|           1.0|  (2,[0],[1.0])|  (2,[1],[1.0])|\n",
      "|           2.0|           0.0|      (2,[],[])|  (2,[0],[1.0])|\n",
      "+--------------+--------------+---------------+---------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(DataFrame[categoryIndex1: double, categoryIndex2: double, onehot_encoded1: vector, onehot_encoded2: vector],\n",
       " None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "        (0.0, 1.0),\n",
    "        (1.0, 0.0),\n",
    "        (2.0, 1.0),\n",
    "        (0.0, 2.0),\n",
    "        (0.0, 1.0),\n",
    "        (2.0, 0.0)\n",
    "    ],\n",
    "    ['categoryIndex1', 'categoryIndex2']\n",
    ")\n",
    "encoder = OneHotEncoder(\n",
    "    dropLast=True, \n",
    "    inputCols=['categoryIndex1', 'categoryIndex2'],\n",
    "    outputCols=['onehot_encoded1', 'onehot_encoded2'],\n",
    ")\n",
    "encoder_model = encoder.fit(df)\n",
    "encoded_df = encoder_model.transform(df)\n",
    "encoded_df, encoded_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---------+\n",
      "| id|category1|category2|\n",
      "+---+---------+---------+\n",
      "|  0|        a|        A|\n",
      "|  1|        b|        A|\n",
      "|  2|        c|        K|\n",
      "|  3|        a|        D|\n",
      "|  4|        a|        C|\n",
      "|  5|        c|        B|\n",
      "+---+---------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/06/06 00:28:18 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'b74a552072e446b3a80810640f088e1c', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current pyspark.ml workflow\n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: Column category1 must be of type numeric but was actually of type string.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m df\u001b[39m.\u001b[39mshow()\n\u001b[1;32m      7\u001b[0m encoder \u001b[39m=\u001b[39m OneHotEncoder(inputCols\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mcategory1\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcategory2\u001b[39m\u001b[39m'\u001b[39m], outputCols\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39monehot_encoded1\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39monehot_encoded2\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m----> 8\u001b[0m encoded_model \u001b[39m=\u001b[39m encoder\u001b[39m.\u001b[39;49mfit(df)\n",
      "File \u001b[0;32m/workspace/apache-spark-ml/.venv/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py:554\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    552\u001b[0m     patch_function\u001b[39m.\u001b[39mcall(call_original, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    553\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 554\u001b[0m     patch_function(call_original, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    556\u001b[0m session\u001b[39m.\u001b[39mstate \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msucceeded\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    558\u001b[0m try_log_autologging_event(\n\u001b[1;32m    559\u001b[0m     AutologgingEventLogger\u001b[39m.\u001b[39mget_logger()\u001b[39m.\u001b[39mlog_patch_function_success,\n\u001b[1;32m    560\u001b[0m     session,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    564\u001b[0m     kwargs,\n\u001b[1;32m    565\u001b[0m )\n",
      "File \u001b[0;32m/workspace/apache-spark-ml/.venv/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py:254\u001b[0m, in \u001b[0;36mwith_managed_run.<locals>.patch_with_managed_run\u001b[0;34m(original, *args, **kwargs)\u001b[0m\n\u001b[1;32m    251\u001b[0m     managed_run \u001b[39m=\u001b[39m create_managed_run()\n\u001b[1;32m    253\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 254\u001b[0m     result \u001b[39m=\u001b[39m patch_function(original, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    255\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mException\u001b[39;00m, \u001b[39mKeyboardInterrupt\u001b[39;00m):\n\u001b[1;32m    256\u001b[0m     \u001b[39m# In addition to standard Python exceptions, handle keyboard interrupts to ensure\u001b[39;00m\n\u001b[1;32m    257\u001b[0m     \u001b[39m# that runs are terminated if a user prematurely interrupts training execution\u001b[39;00m\n\u001b[1;32m    258\u001b[0m     \u001b[39m# (e.g. via sigint / ctrl-c)\u001b[39;00m\n\u001b[1;32m    259\u001b[0m     \u001b[39mif\u001b[39;00m managed_run:\n",
      "File \u001b[0;32m/workspace/apache-spark-ml/.venv/lib/python3.8/site-packages/mlflow/pyspark/ml/__init__.py:1114\u001b[0m, in \u001b[0;36mautolog.<locals>.patched_fit\u001b[0;34m(original, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1112\u001b[0m \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mshould_log():\n\u001b[1;32m   1113\u001b[0m     \u001b[39mwith\u001b[39;00m _AUTOLOGGING_METRICS_MANAGER\u001b[39m.\u001b[39mdisable_log_post_training_metrics():\n\u001b[0;32m-> 1114\u001b[0m         fit_result \u001b[39m=\u001b[39m fit_mlflow(original, \u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1115\u001b[0m     \u001b[39m# In some cases the `fit_result` may be an iterator of spark models.\u001b[39;00m\n\u001b[1;32m   1116\u001b[0m     \u001b[39mif\u001b[39;00m should_log_post_training_metrics \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(fit_result, Model):\n",
      "File \u001b[0;32m/workspace/apache-spark-ml/.venv/lib/python3.8/site-packages/mlflow/pyspark/ml/__init__.py:1086\u001b[0m, in \u001b[0;36mautolog.<locals>.fit_mlflow\u001b[0;34m(original, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1083\u001b[0m \u001b[39m# Do not perform autologging on direct calls to fit() for featurizers.\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m \u001b[39m# Note that featurizers will be autologged when they're fit as part of a Pipeline.\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m _get_fully_qualified_class_name(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mpyspark.ml.feature.\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1086\u001b[0m     \u001b[39mreturn\u001b[39;00m original(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1087\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(params, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m   1088\u001b[0m     \u001b[39m# skip the case params is a list or tuple, this case it will call\u001b[39;00m\n\u001b[1;32m   1089\u001b[0m     \u001b[39m# fitMultiple and return a model iterator\u001b[39;00m\n\u001b[1;32m   1090\u001b[0m     _logger\u001b[39m.\u001b[39mwarning(_get_warning_msg_for_fit_call_with_a_list_of_params(\u001b[39mself\u001b[39m))\n",
      "File \u001b[0;32m/workspace/apache-spark-ml/.venv/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py:535\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original\u001b[0;34m(*og_args, **og_kwargs)\u001b[0m\n\u001b[1;32m    532\u001b[0m         original_result \u001b[39m=\u001b[39m original(\u001b[39m*\u001b[39m_og_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_og_kwargs)\n\u001b[1;32m    533\u001b[0m         \u001b[39mreturn\u001b[39;00m original_result\n\u001b[0;32m--> 535\u001b[0m \u001b[39mreturn\u001b[39;00m call_original_fn_with_event_logging(_original_fn, og_args, og_kwargs)\n",
      "File \u001b[0;32m/workspace/apache-spark-ml/.venv/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py:470\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original_fn_with_event_logging\u001b[0;34m(original_fn, og_args, og_kwargs)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     try_log_autologging_event(\n\u001b[1;32m    463\u001b[0m         AutologgingEventLogger\u001b[39m.\u001b[39mget_logger()\u001b[39m.\u001b[39mlog_original_function_start,\n\u001b[1;32m    464\u001b[0m         session,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    468\u001b[0m         og_kwargs,\n\u001b[1;32m    469\u001b[0m     )\n\u001b[0;32m--> 470\u001b[0m     original_fn_result \u001b[39m=\u001b[39m original_fn(\u001b[39m*\u001b[39;49mog_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mog_kwargs)\n\u001b[1;32m    472\u001b[0m     try_log_autologging_event(\n\u001b[1;32m    473\u001b[0m         AutologgingEventLogger\u001b[39m.\u001b[39mget_logger()\u001b[39m.\u001b[39mlog_original_function_success,\n\u001b[1;32m    474\u001b[0m         session,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    478\u001b[0m         og_kwargs,\n\u001b[1;32m    479\u001b[0m     )\n\u001b[1;32m    480\u001b[0m     \u001b[39mreturn\u001b[39;00m original_fn_result\n",
      "File \u001b[0;32m/workspace/apache-spark-ml/.venv/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py:532\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original.<locals>._original_fn\u001b[0;34m(*_og_args, **_og_kwargs)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[39m# Show all non-MLflow warnings as normal (i.e. not as event logs)\u001b[39;00m\n\u001b[1;32m    525\u001b[0m \u001b[39m# during original function execution, even if silent mode is enabled\u001b[39;00m\n\u001b[1;32m    526\u001b[0m \u001b[39m# (`silent=True`), since these warnings originate from the ML framework\u001b[39;00m\n\u001b[1;32m    527\u001b[0m \u001b[39m# or one of its dependencies and are likely relevant to the caller\u001b[39;00m\n\u001b[1;32m    528\u001b[0m \u001b[39mwith\u001b[39;00m set_non_mlflow_warnings_behavior_for_current_thread(\n\u001b[1;32m    529\u001b[0m     disable_warnings\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    530\u001b[0m     reroute_warnings\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    531\u001b[0m ):\n\u001b[0;32m--> 532\u001b[0m     original_result \u001b[39m=\u001b[39m original(\u001b[39m*\u001b[39;49m_og_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m_og_kwargs)\n\u001b[1;32m    533\u001b[0m     \u001b[39mreturn\u001b[39;00m original_result\n",
      "File \u001b[0;32m/workspace/apache-spark-ml/.venv/lib/python3.8/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[1;32m    206\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/workspace/apache-spark-ml/.venv/lib/python3.8/site-packages/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fit\u001b[39m(\u001b[39mself\u001b[39m, dataset: DataFrame) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_java(dataset)\n\u001b[1;32m    382\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    383\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m/workspace/apache-spark-ml/.venv/lib/python3.8/site-packages/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_java_obj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_java_obj\u001b[39m.\u001b[39;49mfit(dataset\u001b[39m.\u001b[39;49m_jdf)\n",
      "File \u001b[0;32m/workspace/apache-spark-ml/.venv/lib/python3.8/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m/workspace/apache-spark-ml/.venv/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: Column category1 must be of type numeric but was actually of type string."
     ]
    }
   ],
   "source": [
    "# error: onehot encoded can only be applied to numeric category value\n",
    "df = spark.createDataFrame(\n",
    "    [(0, 'a', 'A'), (1, 'b', 'A'), (2, 'c', 'K'), (3, 'a', 'D'), (4, 'a', 'C'), (5, 'c', 'B')],\n",
    "    ['id', 'category1', 'category2']\n",
    ")\n",
    "df.show()\n",
    "encoder = OneHotEncoder(inputCols=['category1', 'category2'], outputCols=['onehot_encoded1', 'onehot_encoded2'])\n",
    "encoded_model = encoder.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/06/05 23:57:21 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '182c5f821f5345dfaea270ae52ba4380', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current pyspark.ml workflow\n",
      "2023/06/05 23:57:22 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '5261b9bee2804921b5a11c71499d5f2b', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current pyspark.ml workflow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---------+--------------+--------------+---------------+---------------+\n",
      "| id|category1|category2|label_encoded1|label_encoded2|onehot_encoded1|onehot_encoded2|\n",
      "+---+---------+---------+--------------+--------------+---------------+---------------+\n",
      "|  0|        a|        A|           0.0|           0.0|  (2,[0],[1.0])|  (4,[0],[1.0])|\n",
      "|  1|        b|        A|           2.0|           0.0|      (2,[],[])|  (4,[0],[1.0])|\n",
      "|  2|        c|        K|           1.0|           4.0|  (2,[1],[1.0])|      (4,[],[])|\n",
      "|  3|        a|        D|           0.0|           3.0|  (2,[0],[1.0])|  (4,[3],[1.0])|\n",
      "|  4|        a|        C|           0.0|           2.0|  (2,[0],[1.0])|  (4,[2],[1.0])|\n",
      "|  5|        c|        B|           1.0|           1.0|  (2,[1],[1.0])|  (4,[1],[1.0])|\n",
      "+---+---------+---------+--------------+--------------+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [(0, 'a', 'A'), (1, 'b', 'A'), (2, 'c', 'K'), (3, 'a', 'D'), (4, 'a', 'C'), (5, 'c', 'B')],\n",
    "    ['id', 'category1', 'category2']\n",
    ")\n",
    "label_encoder = StringIndexer(inputCols=['category1', 'category2'], outputCols=['label_encoded1', 'label_encoded2'])\n",
    "label_encoded_df = label_encoder.fit(df).transform(df)\n",
    "onehot_encoder = OneHotEncoder(inputCols=['label_encoded1', 'label_encoded2'], outputCols=['onehot_encoded1', 'onehot_encoded2'])\n",
    "onehot_encoded_df = onehot_encoder.fit(label_encoded_df).transform(label_encoded_df)\n",
    "onehot_encoded_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/06/06 00:46:39 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '1306d74a610c4186aa119d4cc5178775', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current pyspark.ml workflow\n",
      "23/06/06 00:46:40 WARN StringIndexerModel: Input column category1 does not exist during transformation. Skip StringIndexerModel for this column.\n",
      "23/06/06 00:46:40 WARN StringIndexerModel: Input column category2 does not exist during transformation. Skip StringIndexerModel for this column.\n",
      "2023/06/06 00:46:42 WARNING mlflow.pyspark.ml: Model outputs contain unsupported Spark data types: [StructField('onehot_encoded1', VectorUDT(), True), StructField('onehot_encoded2', VectorUDT(), True)]. Output schema is not be logged.\n",
      "23/06/06 00:46:42 ERROR Instrumentation: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"mlflow-artifacts\"\n",
      "\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)\n",
      "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n",
      "\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n",
      "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n",
      "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
      "\tat org.apache.spark.ml.util.FileSystemOverwrite.handleOverwrite(ReadWrite.scala:673)\n",
      "\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:167)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.super$save(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$4(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent(events.scala:174)\n",
      "\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent$(events.scala:169)\n",
      "\tat org.apache.spark.ml.util.Instrumentation.withSaveInstanceEvent(Instrumentation.scala:42)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3$adapted(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.save(Pipeline.scala:344)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "2023/06/06 00:47:10 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '18d22d0906f14193971e8fbc78103d2a', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current pyspark.ml workflow\n",
      "23/06/06 00:47:12 WARN StringIndexerModel: Input column category1 does not exist during transformation. Skip StringIndexerModel for this column.\n",
      "23/06/06 00:47:12 WARN StringIndexerModel: Input column category2 does not exist during transformation. Skip StringIndexerModel for this column.\n",
      "2023/06/06 00:47:13 WARNING mlflow.pyspark.ml: Model outputs contain unsupported Spark data types: [StructField('onehot_encoded1', VectorUDT(), True), StructField('onehot_encoded2', VectorUDT(), True)]. Output schema is not be logged.\n",
      "23/06/06 00:47:13 ERROR Instrumentation: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"mlflow-artifacts\"\n",
      "\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)\n",
      "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n",
      "\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n",
      "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n",
      "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
      "\tat org.apache.spark.ml.util.FileSystemOverwrite.handleOverwrite(ReadWrite.scala:673)\n",
      "\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:167)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.super$save(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$4(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent(events.scala:174)\n",
      "\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent$(events.scala:169)\n",
      "\tat org.apache.spark.ml.util.Instrumentation.withSaveInstanceEvent(Instrumentation.scala:42)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3$adapted(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.save(Pipeline.scala:344)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---------+--------------+--------------+---------------+---------------+\n",
      "| id|category1|category2|label_encoded1|label_encoded2|onehot_encoded1|onehot_encoded2|\n",
      "+---+---------+---------+--------------+--------------+---------------+---------------+\n",
      "|  0|        a|        A|           0.0|           0.0|  (2,[0],[1.0])|  (4,[0],[1.0])|\n",
      "|  1|        b|        A|           2.0|           0.0|      (2,[],[])|  (4,[0],[1.0])|\n",
      "|  2|        c|        K|           1.0|           4.0|  (2,[1],[1.0])|      (4,[],[])|\n",
      "|  3|        a|        D|           0.0|           3.0|  (2,[0],[1.0])|  (4,[3],[1.0])|\n",
      "|  4|        a|        C|           0.0|           2.0|  (2,[0],[1.0])|  (4,[2],[1.0])|\n",
      "|  5|        c|        B|           1.0|           1.0|  (2,[1],[1.0])|  (4,[1],[1.0])|\n",
      "+---+---------+---------+--------------+--------------+---------------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "stage_1 = StringIndexer(inputCols=['category1', 'category2'], outputCols=['label_encoded1', 'label_encoded2'])\n",
    "stage_2 = OneHotEncoder(inputCols=['label_encoded1', 'label_encoded2'], outputCols=['onehot_encoded1', 'onehot_encoded2'])\n",
    "\n",
    "pipeline = Pipeline(stages=[stage_1, stage_2])\n",
    "# pipeline_model = pipeline.fit(df)\n",
    "onehot_encoded_df = pipeline.fit(df).transform(df)\n",
    "\n",
    "onehot_encoded_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexerModel: uid=StringIndexer_9b7adfe11335, handleInvalid=error, numInputCols=2, numOutputCols=2,\n",
       " OneHotEncoderModel: uid=OneHotEncoder_4dcf732c2798, dropLast=true, handleInvalid=error, numInputCols=2, numOutputCols=2]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipeline_model.stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-----+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|label|\n",
      "+------------+-----------+------------+-----------+-----+\n",
      "|         5.1|        3.5|         1.4|        0.2|    0|\n",
      "|         4.9|        3.0|         1.4|        0.2|    0|\n",
      "|         4.7|        3.2|         1.3|        0.2|    0|\n",
      "|         4.6|        3.1|         1.5|        0.2|    0|\n",
      "|         5.0|        3.6|         1.4|        0.2|    0|\n",
      "|         5.4|        3.9|         1.7|        0.4|    0|\n",
      "|         4.6|        3.4|         1.4|        0.3|    0|\n",
      "|         5.0|        3.4|         1.5|        0.2|    0|\n",
      "|         4.4|        2.9|         1.4|        0.2|    0|\n",
      "|         4.9|        3.1|         1.5|        0.1|    0|\n",
      "|         5.4|        3.7|         1.5|        0.2|    0|\n",
      "|         4.8|        3.4|         1.6|        0.2|    0|\n",
      "|         4.8|        3.0|         1.4|        0.1|    0|\n",
      "|         4.3|        3.0|         1.1|        0.1|    0|\n",
      "|         5.8|        4.0|         1.2|        0.2|    0|\n",
      "|         5.7|        4.4|         1.5|        0.4|    0|\n",
      "|         5.4|        3.9|         1.3|        0.4|    0|\n",
      "|         5.1|        3.5|         1.4|        0.3|    0|\n",
      "|         5.7|        3.8|         1.7|        0.3|    0|\n",
      "|         5.1|        3.8|         1.5|        0.3|    0|\n",
      "+------------+-----------+------------+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "iris = load_iris()\n",
    "iris_data, iris_label = iris.data, iris.target\n",
    "iris_columns = list(map(lambda n: n.replace('al ', 'al_').replace(' (cm)', ''),iris.feature_names))\n",
    "iris_pdf = pd.DataFrame(iris_data, columns=iris_columns)\n",
    "iris_pdf['label'] = iris_label\n",
    "iris_sdf = spark.createDataFrame(iris_pdf)\n",
    "iris_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/06/06 00:54:46 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'ccf37601492a4605a5233327ff0bea59', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current pyspark.ml workflow\n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: Column sepal_length must be of type class org.apache.spark.ml.linalg.VectorUDT:struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually class org.apache.spark.sql.types.DoubleType$:double.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mml\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature\u001b[39;00m \u001b[39mimport\u001b[39;00m StandardScaler\n\u001b[1;32m      3\u001b[0m standard_scaler \u001b[39m=\u001b[39m StandardScaler(inputCol\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msepal_length\u001b[39m\u001b[39m'\u001b[39m, outputCol\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mscaled_sepal_length\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m standard_scaler_model \u001b[39m=\u001b[39m standard_scaler\u001b[39m.\u001b[39;49mfit(iris_sdf)\n\u001b[1;32m      5\u001b[0m standard_scaled_df \u001b[39m=\u001b[39m standard_scaler_model\u001b[39m.\u001b[39mtransform(iris_sdf)\n\u001b[1;32m      6\u001b[0m standard_scaled_df\u001b[39m.\u001b[39mshow()\n",
      "File \u001b[0;32m/workspace/apache-spark-ml/.venv/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py:554\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    552\u001b[0m     patch_function\u001b[39m.\u001b[39mcall(call_original, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    553\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 554\u001b[0m     patch_function(call_original, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    556\u001b[0m session\u001b[39m.\u001b[39mstate \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msucceeded\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    558\u001b[0m try_log_autologging_event(\n\u001b[1;32m    559\u001b[0m     AutologgingEventLogger\u001b[39m.\u001b[39mget_logger()\u001b[39m.\u001b[39mlog_patch_function_success,\n\u001b[1;32m    560\u001b[0m     session,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    564\u001b[0m     kwargs,\n\u001b[1;32m    565\u001b[0m )\n",
      "File \u001b[0;32m/workspace/apache-spark-ml/.venv/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py:254\u001b[0m, in \u001b[0;36mwith_managed_run.<locals>.patch_with_managed_run\u001b[0;34m(original, *args, **kwargs)\u001b[0m\n\u001b[1;32m    251\u001b[0m     managed_run \u001b[39m=\u001b[39m create_managed_run()\n\u001b[1;32m    253\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 254\u001b[0m     result \u001b[39m=\u001b[39m patch_function(original, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    255\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mException\u001b[39;00m, \u001b[39mKeyboardInterrupt\u001b[39;00m):\n\u001b[1;32m    256\u001b[0m     \u001b[39m# In addition to standard Python exceptions, handle keyboard interrupts to ensure\u001b[39;00m\n\u001b[1;32m    257\u001b[0m     \u001b[39m# that runs are terminated if a user prematurely interrupts training execution\u001b[39;00m\n\u001b[1;32m    258\u001b[0m     \u001b[39m# (e.g. via sigint / ctrl-c)\u001b[39;00m\n\u001b[1;32m    259\u001b[0m     \u001b[39mif\u001b[39;00m managed_run:\n",
      "File \u001b[0;32m/workspace/apache-spark-ml/.venv/lib/python3.8/site-packages/mlflow/pyspark/ml/__init__.py:1114\u001b[0m, in \u001b[0;36mautolog.<locals>.patched_fit\u001b[0;34m(original, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1112\u001b[0m \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mshould_log():\n\u001b[1;32m   1113\u001b[0m     \u001b[39mwith\u001b[39;00m _AUTOLOGGING_METRICS_MANAGER\u001b[39m.\u001b[39mdisable_log_post_training_metrics():\n\u001b[0;32m-> 1114\u001b[0m         fit_result \u001b[39m=\u001b[39m fit_mlflow(original, \u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1115\u001b[0m     \u001b[39m# In some cases the `fit_result` may be an iterator of spark models.\u001b[39;00m\n\u001b[1;32m   1116\u001b[0m     \u001b[39mif\u001b[39;00m should_log_post_training_metrics \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(fit_result, Model):\n",
      "File \u001b[0;32m/workspace/apache-spark-ml/.venv/lib/python3.8/site-packages/mlflow/pyspark/ml/__init__.py:1086\u001b[0m, in \u001b[0;36mautolog.<locals>.fit_mlflow\u001b[0;34m(original, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1083\u001b[0m \u001b[39m# Do not perform autologging on direct calls to fit() for featurizers.\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m \u001b[39m# Note that featurizers will be autologged when they're fit as part of a Pipeline.\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m _get_fully_qualified_class_name(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mpyspark.ml.feature.\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1086\u001b[0m     \u001b[39mreturn\u001b[39;00m original(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1087\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(params, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m   1088\u001b[0m     \u001b[39m# skip the case params is a list or tuple, this case it will call\u001b[39;00m\n\u001b[1;32m   1089\u001b[0m     \u001b[39m# fitMultiple and return a model iterator\u001b[39;00m\n\u001b[1;32m   1090\u001b[0m     _logger\u001b[39m.\u001b[39mwarning(_get_warning_msg_for_fit_call_with_a_list_of_params(\u001b[39mself\u001b[39m))\n",
      "File \u001b[0;32m/workspace/apache-spark-ml/.venv/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py:535\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original\u001b[0;34m(*og_args, **og_kwargs)\u001b[0m\n\u001b[1;32m    532\u001b[0m         original_result \u001b[39m=\u001b[39m original(\u001b[39m*\u001b[39m_og_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_og_kwargs)\n\u001b[1;32m    533\u001b[0m         \u001b[39mreturn\u001b[39;00m original_result\n\u001b[0;32m--> 535\u001b[0m \u001b[39mreturn\u001b[39;00m call_original_fn_with_event_logging(_original_fn, og_args, og_kwargs)\n",
      "File \u001b[0;32m/workspace/apache-spark-ml/.venv/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py:470\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original_fn_with_event_logging\u001b[0;34m(original_fn, og_args, og_kwargs)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     try_log_autologging_event(\n\u001b[1;32m    463\u001b[0m         AutologgingEventLogger\u001b[39m.\u001b[39mget_logger()\u001b[39m.\u001b[39mlog_original_function_start,\n\u001b[1;32m    464\u001b[0m         session,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    468\u001b[0m         og_kwargs,\n\u001b[1;32m    469\u001b[0m     )\n\u001b[0;32m--> 470\u001b[0m     original_fn_result \u001b[39m=\u001b[39m original_fn(\u001b[39m*\u001b[39;49mog_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mog_kwargs)\n\u001b[1;32m    472\u001b[0m     try_log_autologging_event(\n\u001b[1;32m    473\u001b[0m         AutologgingEventLogger\u001b[39m.\u001b[39mget_logger()\u001b[39m.\u001b[39mlog_original_function_success,\n\u001b[1;32m    474\u001b[0m         session,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    478\u001b[0m         og_kwargs,\n\u001b[1;32m    479\u001b[0m     )\n\u001b[1;32m    480\u001b[0m     \u001b[39mreturn\u001b[39;00m original_fn_result\n",
      "File \u001b[0;32m/workspace/apache-spark-ml/.venv/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py:532\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original.<locals>._original_fn\u001b[0;34m(*_og_args, **_og_kwargs)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[39m# Show all non-MLflow warnings as normal (i.e. not as event logs)\u001b[39;00m\n\u001b[1;32m    525\u001b[0m \u001b[39m# during original function execution, even if silent mode is enabled\u001b[39;00m\n\u001b[1;32m    526\u001b[0m \u001b[39m# (`silent=True`), since these warnings originate from the ML framework\u001b[39;00m\n\u001b[1;32m    527\u001b[0m \u001b[39m# or one of its dependencies and are likely relevant to the caller\u001b[39;00m\n\u001b[1;32m    528\u001b[0m \u001b[39mwith\u001b[39;00m set_non_mlflow_warnings_behavior_for_current_thread(\n\u001b[1;32m    529\u001b[0m     disable_warnings\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    530\u001b[0m     reroute_warnings\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    531\u001b[0m ):\n\u001b[0;32m--> 532\u001b[0m     original_result \u001b[39m=\u001b[39m original(\u001b[39m*\u001b[39;49m_og_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m_og_kwargs)\n\u001b[1;32m    533\u001b[0m     \u001b[39mreturn\u001b[39;00m original_result\n",
      "File \u001b[0;32m/workspace/apache-spark-ml/.venv/lib/python3.8/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[1;32m    206\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/workspace/apache-spark-ml/.venv/lib/python3.8/site-packages/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fit\u001b[39m(\u001b[39mself\u001b[39m, dataset: DataFrame) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_java(dataset)\n\u001b[1;32m    382\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    383\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m/workspace/apache-spark-ml/.venv/lib/python3.8/site-packages/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_java_obj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_java_obj\u001b[39m.\u001b[39;49mfit(dataset\u001b[39m.\u001b[39;49m_jdf)\n",
      "File \u001b[0;32m/workspace/apache-spark-ml/.venv/lib/python3.8/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m/workspace/apache-spark-ml/.venv/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: Column sepal_length must be of type class org.apache.spark.ml.linalg.VectorUDT:struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually class org.apache.spark.sql.types.DoubleType$:double."
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "# error: must be a vectorized column\n",
    "standard_scaler = StandardScaler(inputCol='sepal_length', outputCol='scaled_sepal_length')\n",
    "standard_scaler_model = standard_scaler.fit(iris_sdf)\n",
    "standard_scaled_df = standard_scaler_model.transform(iris_sdf)\n",
    "standard_scaled_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-----+-------------------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|label|sepal_length_vector|\n",
      "+------------+-----------+------------+-----------+-----+-------------------+\n",
      "|         5.1|        3.5|         1.4|        0.2|    0|              [5.1]|\n",
      "|         4.9|        3.0|         1.4|        0.2|    0|              [4.9]|\n",
      "|         4.7|        3.2|         1.3|        0.2|    0|              [4.7]|\n",
      "|         4.6|        3.1|         1.5|        0.2|    0|              [4.6]|\n",
      "|         5.0|        3.6|         1.4|        0.2|    0|              [5.0]|\n",
      "|         5.4|        3.9|         1.7|        0.4|    0|              [5.4]|\n",
      "|         4.6|        3.4|         1.4|        0.3|    0|              [4.6]|\n",
      "|         5.0|        3.4|         1.5|        0.2|    0|              [5.0]|\n",
      "|         4.4|        2.9|         1.4|        0.2|    0|              [4.4]|\n",
      "|         4.9|        3.1|         1.5|        0.1|    0|              [4.9]|\n",
      "|         5.4|        3.7|         1.5|        0.2|    0|              [5.4]|\n",
      "|         4.8|        3.4|         1.6|        0.2|    0|              [4.8]|\n",
      "|         4.8|        3.0|         1.4|        0.1|    0|              [4.8]|\n",
      "|         4.3|        3.0|         1.1|        0.1|    0|              [4.3]|\n",
      "|         5.8|        4.0|         1.2|        0.2|    0|              [5.8]|\n",
      "|         5.7|        4.4|         1.5|        0.4|    0|              [5.7]|\n",
      "|         5.4|        3.9|         1.3|        0.4|    0|              [5.4]|\n",
      "|         5.1|        3.5|         1.4|        0.3|    0|              [5.1]|\n",
      "|         5.7|        3.8|         1.7|        0.3|    0|              [5.7]|\n",
      "|         5.1|        3.8|         1.5|        0.3|    0|              [5.1]|\n",
      "+------------+-----------+------------+-----------+-----+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vec_assembler = VectorAssembler(inputCols=['sepal_length'], outputCol='sepal_length_vector')\n",
    "iris_sdf_vectorized = vec_assembler.transform(iris_sdf)\n",
    "iris_sdf_vectorized.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/06/06 00:59:18 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '31cc1d1835ff4fdeaf629134bce7008d', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current pyspark.ml workflow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-----+-------------------+--------------------------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|label|sepal_length_vector|scaled_sepal_length_vector|\n",
      "+------------+-----------+------------+-----------+-----+-------------------+--------------------------+\n",
      "|         5.1|        3.5|         1.4|        0.2|    0|              [5.1]|       [6.158928408838794]|\n",
      "|         4.9|        3.0|         1.4|        0.2|    0|              [4.9]|      [5.9174018045706065]|\n",
      "|         4.7|        3.2|         1.3|        0.2|    0|              [4.7]|       [5.675875200302419]|\n",
      "|         4.6|        3.1|         1.5|        0.2|    0|              [4.6]|       [5.555111898168324]|\n",
      "|         5.0|        3.6|         1.4|        0.2|    0|              [5.0]|         [6.0381651067047]|\n",
      "|         5.4|        3.9|         1.7|        0.4|    0|              [5.4]|       [6.521218315241077]|\n",
      "|         4.6|        3.4|         1.4|        0.3|    0|              [4.6]|       [5.555111898168324]|\n",
      "|         5.0|        3.4|         1.5|        0.2|    0|              [5.0]|         [6.0381651067047]|\n",
      "|         4.4|        2.9|         1.4|        0.2|    0|              [4.4]|       [5.313585293900137]|\n",
      "|         4.9|        3.1|         1.5|        0.1|    0|              [4.9]|      [5.9174018045706065]|\n",
      "|         5.4|        3.7|         1.5|        0.2|    0|              [5.4]|       [6.521218315241077]|\n",
      "|         4.8|        3.4|         1.6|        0.2|    0|              [4.8]|       [5.796638502436513]|\n",
      "|         4.8|        3.0|         1.4|        0.1|    0|              [4.8]|       [5.796638502436513]|\n",
      "|         4.3|        3.0|         1.1|        0.1|    0|              [4.3]|       [5.192821991766042]|\n",
      "|         5.8|        4.0|         1.2|        0.2|    0|              [5.8]|       [7.004271523777453]|\n",
      "|         5.7|        4.4|         1.5|        0.4|    0|              [5.7]|       [6.883508221643359]|\n",
      "|         5.4|        3.9|         1.3|        0.4|    0|              [5.4]|       [6.521218315241077]|\n",
      "|         5.1|        3.5|         1.4|        0.3|    0|              [5.1]|       [6.158928408838794]|\n",
      "|         5.7|        3.8|         1.7|        0.3|    0|              [5.7]|       [6.883508221643359]|\n",
      "|         5.1|        3.8|         1.5|        0.3|    0|              [5.1]|       [6.158928408838794]|\n",
      "+------------+-----------+------------+-----------+-----+-------------------+--------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "standard_scaler = StandardScaler(inputCol='sepal_length_vector', outputCol='scaled_sepal_length_vector')\n",
    "standard_scaler_model = standard_scaler.fit(iris_sdf_vectorized)\n",
    "standard_scaled_df = standard_scaler_model.transform(iris_sdf_vectorized)\n",
    "standard_scaled_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/06/06 01:00:41 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '877e45e99d114e95a86695d7ecd653b5', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current pyspark.ml workflow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-----+-------------------+--------------------------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|label|sepal_length_vector|scaled_sepal_length_vector|\n",
      "+------------+-----------+------------+-----------+-----+-------------------+--------------------------+\n",
      "|         5.1|        3.5|         1.4|        0.2|    0|              [5.1]|      [-0.8976738791967...|\n",
      "|         4.9|        3.0|         1.4|        0.2|    0|              [4.9]|      [-1.1392004834649...|\n",
      "|         4.7|        3.2|         1.3|        0.2|    0|              [4.7]|      [-1.3807270877331...|\n",
      "|         4.6|        3.1|         1.5|        0.2|    0|              [4.6]|      [-1.5014903898672...|\n",
      "|         5.0|        3.6|         1.4|        0.2|    0|              [5.0]|       [-1.01843718133086]|\n",
      "|         5.4|        3.9|         1.7|        0.4|    0|              [5.4]|      [-0.5353839727944...|\n",
      "|         4.6|        3.4|         1.4|        0.3|    0|              [4.6]|      [-1.5014903898672...|\n",
      "|         5.0|        3.4|         1.5|        0.2|    0|              [5.0]|       [-1.01843718133086]|\n",
      "|         4.4|        2.9|         1.4|        0.2|    0|              [4.4]|      [-1.7430169941354...|\n",
      "|         4.9|        3.1|         1.5|        0.1|    0|              [4.9]|      [-1.1392004834649...|\n",
      "|         5.4|        3.7|         1.5|        0.2|    0|              [5.4]|      [-0.5353839727944...|\n",
      "|         4.8|        3.4|         1.6|        0.2|    0|              [4.8]|      [-1.2599637855990...|\n",
      "|         4.8|        3.0|         1.4|        0.1|    0|              [4.8]|      [-1.2599637855990...|\n",
      "|         4.3|        3.0|         1.1|        0.1|    0|              [4.3]|      [-1.8637802962695...|\n",
      "|         5.8|        4.0|         1.2|        0.2|    0|              [5.8]|      [-0.0523307642581...|\n",
      "|         5.7|        4.4|         1.5|        0.4|    0|              [5.7]|      [-0.1730940663922...|\n",
      "|         5.4|        3.9|         1.3|        0.4|    0|              [5.4]|      [-0.5353839727944...|\n",
      "|         5.1|        3.5|         1.4|        0.3|    0|              [5.1]|      [-0.8976738791967...|\n",
      "|         5.7|        3.8|         1.7|        0.3|    0|              [5.7]|      [-0.1730940663922...|\n",
      "|         5.1|        3.8|         1.5|        0.3|    0|              [5.1]|      [-0.8976738791967...|\n",
      "+------------+-----------+------------+-----------+-----+-------------------+--------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "standard_scaler = StandardScaler(inputCol='sepal_length_vector', outputCol='scaled_sepal_length_vector', withMean=True, withStd=True)\n",
    "standard_scaler_model = standard_scaler.fit(iris_sdf_vectorized)\n",
    "standard_scaled_df = standard_scaler_model.transform(iris_sdf_vectorized)\n",
    "standard_scaled_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/06/06 01:06:09 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'e5d4e209a7a44c42b8614fa61fe1cee4', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current pyspark.ml workflow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-----+-----------------+---------------------------------------------------------------------------------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|label|features         |standard_scaled_features                                                         |\n",
      "+------------+-----------+------------+-----------+-----+-----------------+---------------------------------------------------------------------------------+\n",
      "|5.1         |3.5        |1.4         |0.2        |0    |[5.1,3.5,1.4,0.2]|[-0.8976738791967663,1.015601990713633,-1.33575163424152,-1.3110521482051305]    |\n",
      "|4.9         |3.0        |1.4         |0.2        |0    |[4.9,3.0,1.4,0.2]|[-1.1392004834649536,-0.1315388120502606,-1.33575163424152,-1.3110521482051305]  |\n",
      "|4.7         |3.2        |1.3         |0.2        |0    |[4.7,3.2,1.3,0.2]|[-1.3807270877331417,0.3273175090552973,-1.3923992862449774,-1.3110521482051305] |\n",
      "|4.6         |3.1        |1.5         |0.2        |0    |[4.6,3.1,1.5,0.2]|[-1.5014903898672365,0.09788934850251833,-1.279103982238063,-1.3110521482051305] |\n",
      "|5.0         |3.6        |1.4         |0.2        |0    |[5.0,3.6,1.4,0.2]|[-1.01843718133086,1.245030151266412,-1.33575163424152,-1.3110521482051305]      |\n",
      "|5.4         |3.9        |1.7         |0.4        |0    |[5.4,3.9,1.7,0.4]|[-0.5353839727944835,1.9333146329247477,-1.1658086782311485,-1.0486667949952977] |\n",
      "|4.6         |3.4        |1.4         |0.3        |0    |[4.6,3.4,1.4,0.3]|[-1.5014903898672365,0.7861738301608541,-1.33575163424152,-1.179859471600214]    |\n",
      "|5.0         |3.4        |1.5         |0.2        |0    |[5.0,3.4,1.5,0.2]|[-1.01843718133086,0.7861738301608541,-1.279103982238063,-1.3110521482051305]    |\n",
      "|4.4         |2.9        |1.4         |0.2        |0    |[4.4,2.9,1.4,0.2]|[-1.7430169941354237,-0.3609669726030395,-1.33575163424152,-1.3110521482051305]  |\n",
      "|4.9         |3.1        |1.5         |0.1        |0    |[4.9,3.1,1.5,0.1]|[-1.1392004834649536,0.09788934850251833,-1.279103982238063,-1.4422448248100466] |\n",
      "|5.4         |3.7        |1.5         |0.2        |0    |[5.4,3.7,1.5,0.2]|[-0.5353839727944835,1.474458311819191,-1.279103982238063,-1.3110521482051305]   |\n",
      "|4.8         |3.4        |1.6         |0.2        |0    |[4.8,3.4,1.6,0.2]|[-1.2599637855990482,0.7861738301608541,-1.2224563302346056,-1.3110521482051305] |\n",
      "|4.8         |3.0        |1.4         |0.1        |0    |[4.8,3.0,1.4,0.1]|[-1.2599637855990482,-0.1315388120502606,-1.33575163424152,-1.4422448248100466]  |\n",
      "|4.3         |3.0        |1.1         |0.1        |0    |[4.3,3.0,1.1,0.1]|[-1.8637802962695182,-0.1315388120502606,-1.5056945902518917,-1.4422448248100466]|\n",
      "|5.8         |4.0        |1.2         |0.2        |0    |[5.8,4.0,1.2,0.2]|[-0.05233076425810808,2.1627427934775265,-1.4490469382484343,-1.3110521482051305]|\n",
      "|5.7         |4.4        |1.5         |0.4        |0    |[5.7,4.4,1.5,0.4]|[-0.17309406639220165,3.080455435688642,-1.279103982238063,-1.0486667949952977]  |\n",
      "|5.4         |3.9        |1.3         |0.4        |0    |[5.4,3.9,1.3,0.4]|[-0.5353839727944835,1.9333146329247477,-1.3923992862449774,-1.0486667949952977] |\n",
      "|5.1         |3.5        |1.4         |0.3        |0    |[5.1,3.5,1.4,0.3]|[-0.8976738791967663,1.015601990713633,-1.33575163424152,-1.179859471600214]     |\n",
      "|5.7         |3.8        |1.7         |0.3        |0    |[5.7,3.8,1.7,0.3]|[-0.17309406639220165,1.7038864723719689,-1.1658086782311485,-1.179859471600214] |\n",
      "|5.1         |3.8        |1.5         |0.3        |0    |[5.1,3.8,1.5,0.3]|[-0.8976738791967663,1.7038864723719689,-1.279103982238063,-1.179859471600214]   |\n",
      "+------------+-----------+------------+-----------+-----+-----------------+---------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vec_assembler = VectorAssembler(inputCols=iris_columns, outputCol='features')\n",
    "standard_scaler = StandardScaler(inputCol='features', outputCol='standard_scaled_features', withMean=True, withStd=True)\n",
    "\n",
    "iris_sdf_vectorized = vec_assembler.transform(iris_sdf)\n",
    "standard_scaled_df = standard_scaler.fit(iris_sdf_vectorized).transform(iris_sdf_vectorized)\n",
    "standard_scaled_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/06/06 01:07:19 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'b3fbb1874a544aeb860c45a27be26a0b', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current pyspark.ml workflow\n",
      "2023/06/06 01:07:20 WARNING mlflow.pyspark.ml: Model outputs contain unsupported Spark data types: [StructField('features', VectorUDT(), True), StructField('standard_scaled_features', VectorUDT(), True)]. Output schema is not be logged.\n",
      "23/06/06 01:07:20 ERROR Instrumentation: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"mlflow-artifacts\"\n",
      "\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)\n",
      "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n",
      "\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n",
      "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n",
      "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
      "\tat org.apache.spark.ml.util.FileSystemOverwrite.handleOverwrite(ReadWrite.scala:673)\n",
      "\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:167)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.super$save(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$4(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent(events.scala:174)\n",
      "\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent$(events.scala:169)\n",
      "\tat org.apache.spark.ml.util.Instrumentation.withSaveInstanceEvent(Instrumentation.scala:42)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3$adapted(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.save(Pipeline.scala:344)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-----+-----------------+---------------------------------------------------------------------------------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|label|features         |standard_scaled_features                                                         |\n",
      "+------------+-----------+------------+-----------+-----+-----------------+---------------------------------------------------------------------------------+\n",
      "|5.1         |3.5        |1.4         |0.2        |0    |[5.1,3.5,1.4,0.2]|[-0.8976738791967663,1.015601990713633,-1.33575163424152,-1.3110521482051305]    |\n",
      "|4.9         |3.0        |1.4         |0.2        |0    |[4.9,3.0,1.4,0.2]|[-1.1392004834649536,-0.1315388120502606,-1.33575163424152,-1.3110521482051305]  |\n",
      "|4.7         |3.2        |1.3         |0.2        |0    |[4.7,3.2,1.3,0.2]|[-1.3807270877331417,0.3273175090552973,-1.3923992862449774,-1.3110521482051305] |\n",
      "|4.6         |3.1        |1.5         |0.2        |0    |[4.6,3.1,1.5,0.2]|[-1.5014903898672365,0.09788934850251833,-1.279103982238063,-1.3110521482051305] |\n",
      "|5.0         |3.6        |1.4         |0.2        |0    |[5.0,3.6,1.4,0.2]|[-1.01843718133086,1.245030151266412,-1.33575163424152,-1.3110521482051305]      |\n",
      "|5.4         |3.9        |1.7         |0.4        |0    |[5.4,3.9,1.7,0.4]|[-0.5353839727944835,1.9333146329247477,-1.1658086782311485,-1.0486667949952977] |\n",
      "|4.6         |3.4        |1.4         |0.3        |0    |[4.6,3.4,1.4,0.3]|[-1.5014903898672365,0.7861738301608541,-1.33575163424152,-1.179859471600214]    |\n",
      "|5.0         |3.4        |1.5         |0.2        |0    |[5.0,3.4,1.5,0.2]|[-1.01843718133086,0.7861738301608541,-1.279103982238063,-1.3110521482051305]    |\n",
      "|4.4         |2.9        |1.4         |0.2        |0    |[4.4,2.9,1.4,0.2]|[-1.7430169941354237,-0.3609669726030395,-1.33575163424152,-1.3110521482051305]  |\n",
      "|4.9         |3.1        |1.5         |0.1        |0    |[4.9,3.1,1.5,0.1]|[-1.1392004834649536,0.09788934850251833,-1.279103982238063,-1.4422448248100466] |\n",
      "|5.4         |3.7        |1.5         |0.2        |0    |[5.4,3.7,1.5,0.2]|[-0.5353839727944835,1.474458311819191,-1.279103982238063,-1.3110521482051305]   |\n",
      "|4.8         |3.4        |1.6         |0.2        |0    |[4.8,3.4,1.6,0.2]|[-1.2599637855990482,0.7861738301608541,-1.2224563302346056,-1.3110521482051305] |\n",
      "|4.8         |3.0        |1.4         |0.1        |0    |[4.8,3.0,1.4,0.1]|[-1.2599637855990482,-0.1315388120502606,-1.33575163424152,-1.4422448248100466]  |\n",
      "|4.3         |3.0        |1.1         |0.1        |0    |[4.3,3.0,1.1,0.1]|[-1.8637802962695182,-0.1315388120502606,-1.5056945902518917,-1.4422448248100466]|\n",
      "|5.8         |4.0        |1.2         |0.2        |0    |[5.8,4.0,1.2,0.2]|[-0.05233076425810808,2.1627427934775265,-1.4490469382484343,-1.3110521482051305]|\n",
      "|5.7         |4.4        |1.5         |0.4        |0    |[5.7,4.4,1.5,0.4]|[-0.17309406639220165,3.080455435688642,-1.279103982238063,-1.0486667949952977]  |\n",
      "|5.4         |3.9        |1.3         |0.4        |0    |[5.4,3.9,1.3,0.4]|[-0.5353839727944835,1.9333146329247477,-1.3923992862449774,-1.0486667949952977] |\n",
      "|5.1         |3.5        |1.4         |0.3        |0    |[5.1,3.5,1.4,0.3]|[-0.8976738791967663,1.015601990713633,-1.33575163424152,-1.179859471600214]     |\n",
      "|5.7         |3.8        |1.7         |0.3        |0    |[5.7,3.8,1.7,0.3]|[-0.17309406639220165,1.7038864723719689,-1.1658086782311485,-1.179859471600214] |\n",
      "|5.1         |3.8        |1.5         |0.3        |0    |[5.1,3.8,1.5,0.3]|[-0.8976738791967663,1.7038864723719689,-1.279103982238063,-1.179859471600214]   |\n",
      "+------------+-----------+------------+-----------+-----+-----------------+---------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(stages=[vec_assembler, standard_scaler])\n",
    "standard_scaled_df = pipeline.fit(iris_sdf).transform(iris_sdf)\n",
    "standard_scaled_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/06/06 01:10:19 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '6516a275ba7f47ceb2b947d3e5278c37', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current pyspark.ml workflow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-----+-----------------+----------------------------------------------------------------------------------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|label|features         |minmax_scaled_features                                                            |\n",
      "+------------+-----------+------------+-----------+-----+-----------------+----------------------------------------------------------------------------------+\n",
      "|5.1         |3.5        |1.4         |0.2        |0    |[5.1,3.5,1.4,0.2]|[0.22222222222222213,0.625,0.06779661016949151,0.04166666666666667]               |\n",
      "|4.9         |3.0        |1.4         |0.2        |0    |[4.9,3.0,1.4,0.2]|[0.1666666666666668,0.41666666666666663,0.06779661016949151,0.04166666666666667]  |\n",
      "|4.7         |3.2        |1.3         |0.2        |0    |[4.7,3.2,1.3,0.2]|[0.11111111111111119,0.5,0.05084745762711865,0.04166666666666667]                 |\n",
      "|4.6         |3.1        |1.5         |0.2        |0    |[4.6,3.1,1.5,0.2]|[0.08333333333333327,0.4583333333333333,0.0847457627118644,0.04166666666666667]   |\n",
      "|5.0         |3.6        |1.4         |0.2        |0    |[5.0,3.6,1.4,0.2]|[0.19444444444444448,0.6666666666666666,0.06779661016949151,0.04166666666666667]  |\n",
      "|5.4         |3.9        |1.7         |0.4        |0    |[5.4,3.9,1.7,0.4]|[0.30555555555555564,0.7916666666666665,0.11864406779661016,0.12500000000000003]  |\n",
      "|4.6         |3.4        |1.4         |0.3        |0    |[4.6,3.4,1.4,0.3]|[0.08333333333333327,0.5833333333333333,0.06779661016949151,0.08333333333333333]  |\n",
      "|5.0         |3.4        |1.5         |0.2        |0    |[5.0,3.4,1.5,0.2]|[0.19444444444444448,0.5833333333333333,0.0847457627118644,0.04166666666666667]   |\n",
      "|4.4         |2.9        |1.4         |0.2        |0    |[4.4,2.9,1.4,0.2]|[0.027777777777777922,0.37499999999999994,0.06779661016949151,0.04166666666666667]|\n",
      "|4.9         |3.1        |1.5         |0.1        |0    |[4.9,3.1,1.5,0.1]|[0.1666666666666668,0.4583333333333333,0.0847457627118644,0.0]                    |\n",
      "|5.4         |3.7        |1.5         |0.2        |0    |[5.4,3.7,1.5,0.2]|[0.30555555555555564,0.7083333333333334,0.0847457627118644,0.04166666666666667]   |\n",
      "|4.8         |3.4        |1.6         |0.2        |0    |[4.8,3.4,1.6,0.2]|[0.13888888888888887,0.5833333333333333,0.1016949152542373,0.04166666666666667]   |\n",
      "|4.8         |3.0        |1.4         |0.1        |0    |[4.8,3.0,1.4,0.1]|[0.13888888888888887,0.41666666666666663,0.06779661016949151,0.0]                 |\n",
      "|4.3         |3.0        |1.1         |0.1        |0    |[4.3,3.0,1.1,0.1]|[0.0,0.41666666666666663,0.016949152542372895,0.0]                                |\n",
      "|5.8         |4.0        |1.2         |0.2        |0    |[5.8,4.0,1.2,0.2]|[0.41666666666666663,0.8333333333333333,0.033898305084745756,0.04166666666666667] |\n",
      "|5.7         |4.4        |1.5         |0.4        |0    |[5.7,4.4,1.5,0.4]|[0.38888888888888895,1.0,0.0847457627118644,0.12500000000000003]                  |\n",
      "|5.4         |3.9        |1.3         |0.4        |0    |[5.4,3.9,1.3,0.4]|[0.30555555555555564,0.7916666666666665,0.05084745762711865,0.12500000000000003]  |\n",
      "|5.1         |3.5        |1.4         |0.3        |0    |[5.1,3.5,1.4,0.3]|[0.22222222222222213,0.625,0.06779661016949151,0.08333333333333333]               |\n",
      "|5.7         |3.8        |1.7         |0.3        |0    |[5.7,3.8,1.7,0.3]|[0.38888888888888895,0.7499999999999999,0.11864406779661016,0.08333333333333333]  |\n",
      "|5.1         |3.8        |1.5         |0.3        |0    |[5.1,3.8,1.5,0.3]|[0.22222222222222213,0.7499999999999999,0.0847457627118644,0.08333333333333333]   |\n",
      "+------------+-----------+------------+-----------+-----+-----------------+----------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "vec_assembler = VectorAssembler(inputCols=iris_columns, outputCol='features')\n",
    "iris_sdf_vectorized = vec_assembler.transform(iris_sdf)\n",
    "minmax_scaler = MinMaxScaler(inputCol='features', outputCol='minmax_scaled_features')\n",
    "minmax_scaled_df = minmax_scaler.fit(iris_sdf_vectorized).transform(iris_sdf_vectorized)\n",
    "minmax_scaled_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/06/06 01:11:36 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'a93a20263c494ca9970524f2a34c0a32', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current pyspark.ml workflow\n",
      "2023/06/06 01:11:37 WARNING mlflow.pyspark.ml: Model outputs contain unsupported Spark data types: [StructField('features', VectorUDT(), True), StructField('minmax_scaled_features', VectorUDT(), True)]. Output schema is not be logged.\n",
      "23/06/06 01:11:37 ERROR Instrumentation: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"mlflow-artifacts\"\n",
      "\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)\n",
      "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n",
      "\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n",
      "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n",
      "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
      "\tat org.apache.spark.ml.util.FileSystemOverwrite.handleOverwrite(ReadWrite.scala:673)\n",
      "\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:167)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.super$save(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$4(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent(events.scala:174)\n",
      "\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent$(events.scala:169)\n",
      "\tat org.apache.spark.ml.util.Instrumentation.withSaveInstanceEvent(Instrumentation.scala:42)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3$adapted(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.save(Pipeline.scala:344)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-----+-----------------+----------------------------------------------------------------------------------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|label|features         |minmax_scaled_features                                                            |\n",
      "+------------+-----------+------------+-----------+-----+-----------------+----------------------------------------------------------------------------------+\n",
      "|5.1         |3.5        |1.4         |0.2        |0    |[5.1,3.5,1.4,0.2]|[0.22222222222222213,0.625,0.06779661016949151,0.04166666666666667]               |\n",
      "|4.9         |3.0        |1.4         |0.2        |0    |[4.9,3.0,1.4,0.2]|[0.1666666666666668,0.41666666666666663,0.06779661016949151,0.04166666666666667]  |\n",
      "|4.7         |3.2        |1.3         |0.2        |0    |[4.7,3.2,1.3,0.2]|[0.11111111111111119,0.5,0.05084745762711865,0.04166666666666667]                 |\n",
      "|4.6         |3.1        |1.5         |0.2        |0    |[4.6,3.1,1.5,0.2]|[0.08333333333333327,0.4583333333333333,0.0847457627118644,0.04166666666666667]   |\n",
      "|5.0         |3.6        |1.4         |0.2        |0    |[5.0,3.6,1.4,0.2]|[0.19444444444444448,0.6666666666666666,0.06779661016949151,0.04166666666666667]  |\n",
      "|5.4         |3.9        |1.7         |0.4        |0    |[5.4,3.9,1.7,0.4]|[0.30555555555555564,0.7916666666666665,0.11864406779661016,0.12500000000000003]  |\n",
      "|4.6         |3.4        |1.4         |0.3        |0    |[4.6,3.4,1.4,0.3]|[0.08333333333333327,0.5833333333333333,0.06779661016949151,0.08333333333333333]  |\n",
      "|5.0         |3.4        |1.5         |0.2        |0    |[5.0,3.4,1.5,0.2]|[0.19444444444444448,0.5833333333333333,0.0847457627118644,0.04166666666666667]   |\n",
      "|4.4         |2.9        |1.4         |0.2        |0    |[4.4,2.9,1.4,0.2]|[0.027777777777777922,0.37499999999999994,0.06779661016949151,0.04166666666666667]|\n",
      "|4.9         |3.1        |1.5         |0.1        |0    |[4.9,3.1,1.5,0.1]|[0.1666666666666668,0.4583333333333333,0.0847457627118644,0.0]                    |\n",
      "|5.4         |3.7        |1.5         |0.2        |0    |[5.4,3.7,1.5,0.2]|[0.30555555555555564,0.7083333333333334,0.0847457627118644,0.04166666666666667]   |\n",
      "|4.8         |3.4        |1.6         |0.2        |0    |[4.8,3.4,1.6,0.2]|[0.13888888888888887,0.5833333333333333,0.1016949152542373,0.04166666666666667]   |\n",
      "|4.8         |3.0        |1.4         |0.1        |0    |[4.8,3.0,1.4,0.1]|[0.13888888888888887,0.41666666666666663,0.06779661016949151,0.0]                 |\n",
      "|4.3         |3.0        |1.1         |0.1        |0    |[4.3,3.0,1.1,0.1]|[0.0,0.41666666666666663,0.016949152542372895,0.0]                                |\n",
      "|5.8         |4.0        |1.2         |0.2        |0    |[5.8,4.0,1.2,0.2]|[0.41666666666666663,0.8333333333333333,0.033898305084745756,0.04166666666666667] |\n",
      "|5.7         |4.4        |1.5         |0.4        |0    |[5.7,4.4,1.5,0.4]|[0.38888888888888895,1.0,0.0847457627118644,0.12500000000000003]                  |\n",
      "|5.4         |3.9        |1.3         |0.4        |0    |[5.4,3.9,1.3,0.4]|[0.30555555555555564,0.7916666666666665,0.05084745762711865,0.12500000000000003]  |\n",
      "|5.1         |3.5        |1.4         |0.3        |0    |[5.1,3.5,1.4,0.3]|[0.22222222222222213,0.625,0.06779661016949151,0.08333333333333333]               |\n",
      "|5.7         |3.8        |1.7         |0.3        |0    |[5.7,3.8,1.7,0.3]|[0.38888888888888895,0.7499999999999999,0.11864406779661016,0.08333333333333333]  |\n",
      "|5.1         |3.8        |1.5         |0.3        |0    |[5.1,3.8,1.5,0.3]|[0.22222222222222213,0.7499999999999999,0.0847457627118644,0.08333333333333333]   |\n",
      "+------------+-----------+------------+-----------+-----+-----------------+----------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(stages=[vec_assembler, minmax_scaler])\n",
    "minmax_scaled_df = pipeline.fit(iris_sdf).transform(iris_sdf)\n",
    "minmax_scaled_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
